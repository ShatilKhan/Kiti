{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kiti Autonomous Vehicle - Complete Pipeline\n\n",
        "This notebook combines **Area Marking** and **Optical Flow** analysis for a complete autonomous vehicle computer vision pipeline.\n\n",
        "## Features\n",
        "- **Part 1: Area Marking** - Define region of interest (ROI) for obstacle detection\n",
        "- **Part 2: Object Detection** - YOLO-based object detection\n",
        "- **Part 3: Optical Flow Analysis** - Motion detection and trajectory prediction\n",
        "- **Part 4: Trajectory Prediction** - Linear Regression and Kalman Filter methods\n\n",
        "## Project Structure\n",
        "The production-ready Python modules are available in the `src/` directory:\n",
        "- `src/main.py` - Main pipeline orchestration\n",
        "- `src/video_processor.py` - Video frame extraction\n",
        "- `src/object_detection.py` - YOLO object detection\n",
        "- `src/optical_flow.py` - Optical flow analysis\n",
        "- `config/settings.py` - Centralized configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Part 1: Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install ultralytics opencv-python scikit-learn matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from collections import deque\n\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (for Colab usage)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the folder path for input videos\n",
        "data_path = \"/content/drive/MyDrive/Video\"\n\n",
        "# List video files in the folder\n",
        "video_paths = [os.path.join(data_path, file) for file in os.listdir(data_path) if file.lower().endswith(\".mp4\")]\n",
        "print(\"Available videos:\", video_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Part 2: Area Marking\n\n",
        "Define the central 40% region of interest (ROI) for obstacle detection. Objects within this area are prioritized for autonomous vehicle response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mark_central_area(video_path, output_path=\"marked_video.mp4\"):\n",
        "    \"\"\"\n",
        "    Draw a green box over the central 40% of the video frame.\n",
        "    This defines the region of interest for obstacle detection.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (frame_width, frame_height))\n\n",
        "    start_x = int(0.3 * frame_width)\n",
        "    end_x = int(0.7 * frame_width)\n\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        cv2.rectangle(frame, (start_x, 0), (end_x, frame_height), (0, 255, 0), 3)\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Processed {frame_count} frames. Saved to: {output_path}\")\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Part 3: Motion-Based Obstacle Detection\n\n",
        "Use background subtraction to detect moving objects within the ROI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_obstacles_motion(video_path, output_path=\"obstacle_detected.mp4\"):\n",
        "    \"\"\"\n",
        "    Detect obstacles using background subtraction.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (frame_width, frame_height))\n\n",
        "    start_x = int(0.3 * frame_width)\n",
        "    end_x = int(0.7 * frame_width)\n\n",
        "    fgbg = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=50)\n\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n\n",
        "        fgmask = fgbg.apply(frame)\n",
        "        _, thresh = cv2.threshold(fgmask, 200, 255, cv2.THRESH_BINARY)\n",
        "        thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n",
        "        cv2.rectangle(frame, (start_x, 0), (end_x, frame_height), (0, 255, 0), 3)\n\n",
        "        for cnt in contours:\n",
        "            x, y, w, h = cv2.boundingRect(cnt)\n",
        "            if w * h < 500:\n",
        "                continue\n",
        "            center_x = x + w // 2\n",
        "            if start_x <= center_x <= end_x:\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "                cv2.putText(frame, \"Obstacle Detected\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\n",
        "        out.write(frame)\n\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Obstacle detection complete. Saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Part 4: YOLO Object Detection\n\n",
        "Use YOLOv5/YOLOv8 for accurate object detection with class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n\n",
        "def detect_objects_yolo(video_path, output_path=\"yolo_annotated.mp4\", confidence=0.3):\n",
        "    \"\"\"\n",
        "    Detect objects using YOLOv5 model.\n",
        "    \"\"\"\n",
        "    model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)\n",
        "    model.conf = confidence\n\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (frame_width, frame_height))\n\n",
        "    start_x = int(0.3 * frame_width)\n",
        "    end_x = int(0.7 * frame_width)\n\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n\n",
        "        results = model(frame[..., ::-1])\n",
        "        detections = results.xyxy[0]\n\n",
        "        cv2.rectangle(frame, (start_x, 0), (end_x, frame_height), (0, 255, 0), 3)\n\n",
        "        for *box, conf, cls in detections:\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            class_name = model.names[int(cls)]\n",
        "            center_x = (x1 + x2) // 2\n",
        "            if start_x <= center_x <= end_x:\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                label = f\"{class_name} ({conf:.2f})\"\n",
        "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\n",
        "        out.write(frame)\n\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"YOLO detection complete. Saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Part 5: Optical Flow Analysis\n\n",
        "Track motion between frames using Farneback optical flow algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_optical_flow(video_path, output_path=\"optical_flow.mp4\"):\n",
        "    \"\"\"\n",
        "    Analyze optical flow to track motion and detect moving objects.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n\n",
        "    ret, first_frame = cap.read()\n",
        "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
        "    h, w = first_frame.shape[:2]\n\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 20, (w, h))\n\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n\n",
        "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "        motion_mask = (mag > 1.0).astype(np.uint8) * 255\n\n",
        "        contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n",
        "        largest = None\n",
        "        max_area = 0\n",
        "        for cnt in contours:\n",
        "            if cv2.contourArea(cnt) > 500 and cv2.contourArea(cnt) > max_area:\n",
        "                max_area = cv2.contourArea(cnt)\n",
        "                largest = cnt\n\n",
        "        if largest is not None:\n",
        "            x, y, w_box, h_box = cv2.boundingRect(largest)\n",
        "            cx, cy = x + w_box // 2, y + h_box // 2\n",
        "            cv2.rectangle(frame, (x, y), (x + w_box, y + h_box), (0, 255, 0), 2)\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n\n",
        "        prev_gray = gray\n",
        "        out.write(frame)\n\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Optical flow analysis complete. Saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Part 6: Trajectory Prediction\n\n",
        "Predict future object positions using Linear Regression or Kalman Filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_trajectory_linear(video_path, output_path=\"predicted_path.mp4\"):\n",
        "    \"\"\"\n",
        "    Predict trajectory using Linear Regression based on motion history.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n\n",
        "    ret, first_frame = cap.read()\n",
        "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
        "    h, w = first_frame.shape[:2]\n\n",
        "    trajectory = deque(maxlen=30)\n",
        "    frame_count = 0\n\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 20, (w, h))\n\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "        mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "        motion_mask = (mag > 1.0).astype(np.uint8) * 255\n\n",
        "        contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        largest = None\n",
        "        max_area = 0\n\n",
        "        for cnt in contours:\n",
        "            if cv2.contourArea(cnt) > 500 and cv2.contourArea(cnt) > max_area:\n",
        "                max_area = cv2.contourArea(cnt)\n",
        "                largest = cnt\n\n",
        "        if largest is not None:\n",
        "            x, y, w_box, h_box = cv2.boundingRect(largest)\n",
        "            cx, cy = x + w_box // 2, y + h_box // 2\n\n",
        "            trajectory.append((frame_count, cx, cy))\n",
        "            cv2.rectangle(frame, (x, y), (x + w_box, y + h_box), (0, 255, 0), 2)\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n\n",
        "        if len(trajectory) >= 10:\n",
        "            arr = np.array(trajectory)\n",
        "            times = arr[:, 0].reshape(-1, 1)\n",
        "            xs, ys = arr[:, 1], arr[:, 2]\n\n",
        "            model_x = LinearRegression().fit(times, xs)\n",
        "            model_y = LinearRegression().fit(times, ys)\n\n",
        "            future_times = np.array([[frame_count + i] for i in range(1, 31)])\n",
        "            pred_xs = model_x.predict(future_times).astype(int)\n",
        "            pred_ys = model_y.predict(future_times).astype(int)\n\n",
        "            for px, py in zip(pred_xs, pred_ys):\n",
        "                if 0 <= px < w and 0 <= py < h:\n",
        "                    cv2.circle(frame, (px, py), 2, (255, 0, 0), -1)\n\n",
        "        prev_gray = gray\n",
        "        frame_count += 1\n",
        "        out.write(frame)\n\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Trajectory prediction complete. Saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_trajectory_kalman(video_path, output_path=\"kalman_path.mp4\"):\n",
        "    \"\"\"\n",
        "    Predict trajectory using Kalman Filter for smoothed predictions.\n",
        "    \"\"\"\n",
        "    kf = cv2.KalmanFilter(4, 2)\n",
        "    kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
        "    kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
        "    kf.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03\n",
        "    kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * 1\n\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    ret, first_frame = cap.read()\n",
        "    if not ret:\n",
        "        raise RuntimeError(\"Failed to read video\")\n\n",
        "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
        "    h, w = first_frame.shape[:2]\n\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 20, (w, h))\n",
        "    predicted_points = []\n\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "        mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "        motion_mask = (mag > 1.0).astype(np.uint8) * 255\n\n",
        "        contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        largest = None\n",
        "        max_area = 0\n\n",
        "        for cnt in contours:\n",
        "            if cv2.contourArea(cnt) > 500 and cv2.contourArea(cnt) > max_area:\n",
        "                max_area = cv2.contourArea(cnt)\n",
        "                largest = cnt\n\n",
        "        if largest is not None:\n",
        "            x, y, w_box, h_box = cv2.boundingRect(largest)\n",
        "            cx, cy = x + w_box // 2, y + h_box // 2\n\n",
        "            measurement = np.array([[np.float32(cx)], [np.float32(cy)]])\n",
        "            kf.correct(measurement)\n\n",
        "            prediction = kf.predict()\n",
        "            px, py = int(prediction[0][0]), int(prediction[1][0])\n\n",
        "            predicted_points.append((px, py))\n\n",
        "            cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)\n",
        "            cv2.circle(frame, (px, py), 5, (255, 0, 0), -1)\n\n",
        "            for i in range(1, len(predicted_points)):\n",
        "                cv2.line(frame, predicted_points[i - 1], predicted_points[i], (255, 0, 0), 2)\n\n",
        "        prev_gray = gray\n",
        "        out.write(frame)\n\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Kalman prediction complete. Saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Part 7: Complete Pipeline\n\n",
        "Run the full autonomous vehicle vision pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_complete_pipeline(video_path):\n",
        "    \"\"\"\n",
        "    Run the complete autonomous vehicle vision pipeline.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Kiti Autonomous Vehicle - Vision Pipeline\")\n",
        "    print(\"=\" * 60)\n\n",
        "    print(\"\\n[1/5] Area Marking...\")\n",
        "    mark_central_area(video_path, \"step1_marked.mp4\")\n\n",
        "    print(\"\\n[2/5] Motion-Based Obstacle Detection...\")\n",
        "    detect_obstacles_motion(video_path, \"step2_motion_detection.mp4\")\n\n",
        "    print(\"\\n[3/5] YOLO Object Detection...\")\n",
        "    detect_objects_yolo(video_path, \"step3_yolo_detection.mp4\")\n\n",
        "    print(\"\\n[4/5] Optical Flow Analysis...\")\n",
        "    analyze_optical_flow(video_path, \"step4_optical_flow.mp4\")\n\n",
        "    print(\"\\n[5/5] Trajectory Prediction (Linear + Kalman)...\")\n",
        "    predict_trajectory_linear(video_path, \"step5a_linear_prediction.mp4\")\n",
        "    predict_trajectory_kalman(video_path, \"step5b_kalman_prediction.mp4\")\n\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Pipeline Complete!\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Run the pipeline on a video\n",
        "# video_path = \"/content/drive/MyDrive/Video/clip 2.mp4\"\n",
        "# run_complete_pipeline(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Download Results\n\n",
        "Download the processed videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n\n",
        "# Uncomment to download specific outputs\n",
        "# files.download(\"step1_marked.mp4\")\n",
        "# files.download(\"step3_yolo_detection.mp4\")\n",
        "# files.download(\"step5b_kalman_prediction.mp4\")"
      ]
    }
  ]
}